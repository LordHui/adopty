{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lista import LISTA, make_loss\n",
    "from functions import *\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 5.36e-02\n"
     ]
    }
   ],
   "source": [
    "layers = np.logspace(0, 1.5, 4, dtype=int)\n",
    "n_samples = 1000\n",
    "n_test = 100\n",
    "k = 10\n",
    "p = 3\n",
    "K = np.eye(p)\n",
    "fit_loss = 'logreg'\n",
    "reg = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fit_function, der_function = {\n",
    "                              'l2': (l2_fit, l2_der),\n",
    "                              'logreg': (logreg_fit, logreg_der)\n",
    "                              }[fit_loss]\n",
    "reg_function, prox = {\n",
    "                      'l2': (l2_pen, l2_prox),\n",
    "                      'l1': (l1_pen, l1_prox),\n",
    "                      None: (no_pen, no_prox)\n",
    "                      }[reg]\n",
    "loss = make_loss(fit_function, reg_function)\n",
    "\n",
    "z_true = np.random.randn(p, n_samples)\n",
    "D = np.random.randn(k, p).dot(K)\n",
    "D /= np.linalg.norm(D, axis=0, keepdims=True)\n",
    "\n",
    "sigma = 0.1\n",
    "eps = np.random.randn(k, n_samples)\n",
    "\n",
    "z_test = np.random.randn(p, n_test)\n",
    "eps_test = np.random.randn(k, n_test)\n",
    "X = np.dot(D, z_true) + eps\n",
    "X_test = np.dot(D, z_test) + eps_test\n",
    "if fit_loss == 'logreg':\n",
    "    X = 2 * (X > 0) - 1\n",
    "    X_test = 2 * (X_test > 0) - 1\n",
    "lbda_max = np.max(np.abs(np.dot(D.T, X)))\n",
    "lbda = 0.02 * lbda_max\n",
    "print('lambda = %.2e' % lbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0, loss = 4.992e+00, grad_W = 1.96e-01, grad_l = 0.00e+00\n",
      "it 100, loss = 4.452e+00, grad_W = 1.51e-02, grad_l = 0.00e+00\n",
      "it 200, loss = 4.445e+00, grad_W = 2.71e-03, grad_l = 0.00e+00\n",
      "it 300, loss = 4.445e+00, grad_W = 5.39e-04, grad_l = 0.00e+00\n",
      "it 400, loss = 4.445e+00, grad_W = 1.09e-04, grad_l = 0.00e+00\n",
      "it 500, loss = 4.445e+00, grad_W = 2.22e-05, grad_l = 0.00e+00\n",
      "it 600, loss = 4.445e+00, grad_W = 4.51e-06, grad_l = 0.00e+00\n",
      "it 700, loss = 4.445e+00, grad_W = 9.18e-07, grad_l = 0.00e+00\n",
      "it 800, loss = 4.445e+00, grad_W = 1.87e-07, grad_l = 0.00e+00\n",
      "it 900, loss = 4.445e+00, grad_W = 3.80e-08, grad_l = 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "ista_init = LISTA(D, lbda, 1, fit_loss, reg, variables='both')\n",
    "ista_init.fit(X, l_rate=1., max_iter=1000, verbose=True)\n",
    "init_weights = ista_init.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37841471614553057\n",
      "3.961913120881473\n"
     ]
    }
   ],
   "source": [
    "ista = LISTA(D, lbda, 20000, fit_loss, reg)\n",
    "ista.L /= k\n",
    "L = ista.L\n",
    "z_hat = ista.transform(X)\n",
    "f_min = loss(z_hat, X, D, lbda)\n",
    "\n",
    "print(np.linalg.norm(z_hat - prox(z_hat - np.dot(D.T / L, der_function(np.dot(D, z_hat), X)), lbda / L)))\n",
    "print(f_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 layers\n",
      "3 layers\n",
      "10 layers\n",
      "31 layers\n"
     ]
    }
   ],
   "source": [
    "L_LIST = {}\n",
    "for variables in ['both']:\n",
    "    loss_list = {'ista': [], 'lista': []}\n",
    "    for j, n_layers in enumerate(layers):\n",
    "        print('%d layers' % n_layers)\n",
    "        lista = LISTA(D, lbda, n_layers, fit_loss, reg, variables=variables, learn_levels=True)\n",
    "        lista.weights[:2] = init_weights\n",
    "        loss_list['ista'].append(loss(lista.transform(X), X, D, lbda))\n",
    "        # smart init\n",
    "        if j > 0:\n",
    "            lista.weights[:len(old_weights)] = old_weights\n",
    "        lista.fit(X, l_rate=0.05, max_iter=10000)\n",
    "        loss_list['lista'].append(loss(lista.transform(X), X, D, lbda))\n",
    "        old_weights = deepcopy(lista.weights)\n",
    "    L_LIST[variables] = loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "title = ' %s data fit + %s reg, k=%d, p=%d' % (fit_loss, reg, k, p)\n",
    "plt.title(title)\n",
    "plt.plot(layers, np.array(loss_list['ista']) - f_min, label='Proximal gradient descent with good init')\n",
    "plt.plot(layers, np.array(L_LIST['both']['lista']) - f_min, label='Learned P.G.D.')\n",
    "plt.xlabel('Number of layers / iterations')\n",
    "plt.ylabel('f - f_min')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.savefig(title + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
